


<!DOCTYPE html>
<html lang="en">

<head>
  
  <!-- Required meta tags -->
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />


  <!-- External Javascript libs  -->
  <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
    integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
    integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
    crossorigin="anonymous"></script>


  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
    integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
    integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
    integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>

  <!-- https://developer.snapappointments.com/bootstrap-select/ -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>

  <!-- Library libs -->
  <script src="static/js/typeahead.bundle.js"></script>

  <script src="https://craig.global.ssl.fastly.net/js/mousetrap/mousetrap.min.js?a4098"></script>

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.0/css/all.css"
    integrity="sha384-lZN37f5QGtY3VHgisS14W3ExzMWZxybE1SJSEsQp9S+oqd12jhcu+A56Ebc1zFSJ" crossorigin="anonymous">

  <!-- External CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
    integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

  <!-- External Fonts (no google for china) -->
  <link href="static/css/Lato.css" rel="stylesheet" />
  <link href="static/css/Exo.css" rel="stylesheet" />
  <link href="static/css/Cuprum.css" rel="stylesheet" />

  <link rel="stylesheet" href="static/css/main.css" />
  <link rel="stylesheet" href="static/css/chats-modal.css" />
  <link rel="stylesheet" href="static/css/lazy_load.css" />
  <link rel="stylesheet" href="static/css/typeahead.css" />
  <script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.3.200/build/pdf.min.js"></script>
  <script src="static/js/pdf_render.js"></script>

  <title>ACL2023: BioNLP-ST</title>
  

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="180x180" href="static/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/favicon/favicon-16x16.png">
  <link rel="manifest" href="static/favicon/site.webmanifest">
  <link rel="mask-icon" href="static/favicon/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="static/favicon/favicon.ico">
  <meta name="msapplication-TileColor" content="#2d89ef">
  <meta name="msapplication-config" content="static/favicon/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>

<body>
  <!-- NAV -->
  
  <!-- ('https://2023.aclweb.org/faq/', 'FAQ'), -->
  <!-- ('https://2023.aclweb.org/committees/organization/', 'Organizers'), -->
  <!-- ('https://2023.aclweb.org/sponsors/', 'Sponsors') -->

  <!-- Add back ('livestream.html', 'Livestream'),
    ('about.html', 'Help'),
    ('plenary_sessions.html', 'Plenary'),
    ('livestream.html', 'Livestream'),
    (config.gather_town , 'Gather'),
    for a new conference
    -->

  <nav class="navbar sticky-top navbar-expand-lg navbar-light  bg-emnlp mr-auto customnav" id="main-nav">
    <div class="container">
      <a class="navbar-brand" href="index.html">
        <img class="logo" src="static/images/acl2023/acl-logo-2023.png" height="45px"
          width="auto" alt="ACL 2023" />
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
        <ul class="navbar-nav ml-auto">
          
          <li class="nav-item ">
            
            <a class="nav-link" href="index.html">Home</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="schedule.html">Schedule</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="sessions.html">Sessions</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="papers.html">Papers</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="tutorials.html">Tutorials</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="workshops.html">Workshops</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="map.html">Map</a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://acl.rocket.chat" target="_blank"><u>Chat</u></a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://app.gather.town/app/5OscLswEEuYf2Psb/ACL2023" target="_blank"><u>GatherTown</u></a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://2023.aclweb.org/" target="_blank"><u>Main Site</u></a>
            
          </li>
          
          <li class="nav-item ">
            
            <a class="nav-link" href="https://2023.aclweb.org/downloads/acl2023-handbook.pdf" target="_blank"><u>Handbook</u></a>
            
          </li>
          
        </ul>
      </div>
    </div>
  </nav>
  

  
  <!-- User Overrides -->
   

      
      <div class="container">
        
    <!-- Heading -->
    <div class="heading">
       
    </div>
    <div class="tabs pt-3">
      <!-- Tabs -->
      <div class="tabs pt-3">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      BioNLP-ST
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      Organizers: 
      
      <a href="papers.html?filter=authors&search=Kevin Bretonnel Cohen&program=all"
        class="text-primary link-primary">Kevin Bretonnel Cohen</a>,
      
      <a href="papers.html?filter=authors&search=Dina Demner-Fushman&program=all"
        class="text-primary link-primary">Dina Demner-Fushman</a>,
      
      <a href="papers.html?filter=authors&search=Sophia Ananiadou&program=all"
        class="text-primary link-primary">Sophia Ananiadou</a>,
      
      <a href="papers.html?filter=authors&search=Jun-ichi Tsujii&program=all"
        class="text-primary link-primary">Jun-ichi Tsujii</a>
      
    </h3>
    <div class="text-muted text-center">
      The BioNLP workshop associated with the ACL SIGBIOMED special interest group has established itself as the primary venue for presenting foundational research in language processing for the biological and medical domains. The workshop is running every year since 2002 and continues getting stronger. BioNLP welcomes and encourages work on languages other than English, and inclusion and diversity. BioNLP truly encompasses the breadth of the domain and brings together researchers in bio- and clinical NLP from all over the world. The workshop will continue presenting work on a broad and interesting range of topics in NLP. The interest to biomedical language has broadened significantly due to the COVID-19 pandemic and continues to grow: as access to information becomes easier and more people generate and access health-related text, it becomes clearer that only language technologies can enable and support adequate use of the biomedical text.
    </div>
    <div class="text-center p-3">
      
      <a href="https://aclweb.org/aclwiki/BioNLP_Workshop" target="_blank" class="link-success">
        External Website
      </a>
      


    </div>

  </div>

  <!-- Schedule -->
  <!-- 
-->

  <div class="container" style="background-color:white; padding: 0px;">
    <div class="text-muted text-center">
      You can open
      the
      <a href="https://acl.rocket.chat/channel/workshop-BioNLP-ST" target="_blank">
        #workshop-BioNLP-ST
      </a>
      channel in separate windows.

    </div>

    <div class="row m-2">
      <div class="container" style="background-color:white; padding: 0px;">
        <!-- Chat -->
        <div id="gitter" class="slp">
          <iframe frameborder="0"
            src="https://acl.rocket.chat/channel/workshop-BioNLP-ST?layout=embedded" height="700px"
            width="100%"></iframe>
        </div>
      </div>
    </div>
    <div class="row"><div class="col-12"><h3 class="text-center">Workshop Papers</h3></div></div>
    <div class="row" style='margin: 15px;'>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Multi-Source (Pre-)Training for Cross-Domain Measurement, Unit and Context Extraction
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Yueling Li, Sebastian Martschat, Simone Paolo Ponzetto</h5>
            <p class="card-text">We present a cross-domain approach for automated measurement and context extraction based on pre-trained language models. We construct a multi-source, multi-domain corpus and train an end-to-end extraction pipeline. We then apply multi-source task-adaptive pre-training and fine-tuning to benchmark the cross-domain generalization capability of our model. Further, we conceptualize and apply a task-specific error analysis and derive insights for future work. Our results suggest that multi-source training leads to the best overall results, while single-source training yields the best results for the respective individual domain. While our setup is successful at extracting quantity values and units, more research is needed to improve the extraction of contextual entities. We make the cross-domain corpus used in this work available online.</p>
            <a href="/paper_BioNLP_1.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            GRASUM at BioLaySumm Task 1: Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Domenic Rosati</h5>
            <p class="card-text">Communication of scientific findings to the public is important for keeping non-experts informed of developments such as life-saving medical treatments. However, generating readable lay summaries from scientific documents is challenging, and currently, these summaries suffer from critical factual errors. One popular intervention for improving factuality is using additional external knowledge to provide factual grounding. However, it is unclear how these grounding sources should be retrieved, selected, or integrated, and how supplementary grounding documents might affect the readability or relevance of the generated summaries. We develop a simple method for selecting grounding sources and integrating them with source documents. We then use the BioLaySum summarization dataset to evaluate the effects of different grounding sources on summary quality. We found that grounding source documents improves the relevance and readability of lay summaries but does not improve factuality of lay summaries. This continues to be true in zero-shot summarization settings where we hypothesized that grounding might be even more important for factual lay summaries.</p>
            <a href="/paper_BioNLP_101.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            DeakinNLP at ProbSum 2023: Clinical Progress Note Summarization with Rules and Language ModelsClinical Progress Note Summarization with Rules and Languague Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Ming Liu, Dan Zhang, Weicong Tan, He Zhang</h5>
            <p class="card-text">This paper summarizes two approaches developed for BioNLP2023 workshop task 1A: clinical problem list summarization. We develop two types of methods with either rules or pre-trained language models. In the rule-based summarization model, we leverage UMLS (Unified Medical Language System) and a negation detector to extract text spans to represent the summary. We also fine tune three pre-trained language models (BART, T5 and GPT2) to generate the summaries.  Experiment results show the rule based system returns extractive summaries but lower ROUGE-L score (0.043), while the fine tuned T5 returns a higher ROUGE-L score (0.208).</p>
            <a href="/paper_BioNLP_102.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            TALP-UPC at ProbSum 2023: Fine-tuning and Data Augmentation Strategies for NER
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Neil Torrero, Gerard Sant, Carlos Escolano</h5>
            <p class="card-text">This paper describes the submission of the TALP-UPC team to the Problem List Summarization task from the BioNLP 2023 workshop. This task consists of automatically extracting a list of health issues from the e-health medical record of a given patient. Our submission combines additional steps of data annotationwith finetuning of BERT pre-trained language models. Our experiments focus on the impact of finetuning on different datasets as well as the addition of data augmentation techniques to delay overfitting.</p>
            <a href="/paper_BioNLP_104.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Team:PULSAR at ProbSum 2023:PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients&#39; Problems and Data Augmentation with Black-box Large Language Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Hao Li, Yuping Wu, Viktor Schlegel, Riza Batista-Navarro, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Xiao-Jun Zeng, Daniel Beck, Stefan Winkler, Goran Nenadic</h5>
            <p class="card-text">Medical progress notes play a crucial role in documenting a patient&#39;s hospital journey, including his or her condition, treatment plan, and any updates for healthcare providers. Automatic summarisation of a patient&#39;s problems in the form of a ``problem list&#39;&#39; can aid stakeholders in understanding a patient&#39;s condition, reducing workload and cognitive bias. BioNLP 2023 Shared Task 1A focusses on generating a list of diagnoses and problems from the provider&#39;s progress notes during hospitalisation. In this paper, we introduce our proposed approach to this task, which integrates two complementary components. One component employs large language models (LLMs) for data augmentation; the other is an abstractive summarisation LLM with a novel pre-training objective for generating the patients&#39; problems summarised as a list. Our approach was ranked second among all submissions to the shared task.  The performance of our model on the development and test datasets shows that our approach is more robust on unknown data, with an improvement of up to 3.1 points over the same size of the larger model.</p>
            <a href="/paper_BioNLP_106.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Team Converge at ProbSum 2023: Abstractive Text Summarization of Patient Progress Notes
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Gaurav Kolhatkar, Aditya Paranjape, Omkar Gokhale, Dipali Kadam</h5>
            <p class="card-text">In this paper, we elaborate on our approach for the shared task 1A issued by BioNLP Workshop 2023 titled Problem List Summarization. With an increase in the digitization of health records, a need arises for quick and precise summarization of large amounts of records. With the help of summarization, medical professionals can sieve through multiple records in a short span of time without overlooking any crucial point. We use abstractive text summarization for this task and experiment with multiple state-of-the-art models like Pegasus, BART, and T5, along with various pre-processing and data augmentation techniques to generate summaries from patients&#39; progress notes. For this task, the metric used was the ROUGE-L score. From our experiments, we conclude that Pegasus is the best-performing model on the dataset, achieving a ROUGE-L F1 score of 0.2744 on the test dataset (3rd rank on the leaderboard).</p>
            <a href="/paper_BioNLP_107.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Potsawee Manakul, Yassir Fathullah, Adian Liusie, Vyas Raina, Vatsal Raina, Mark Gales</h5>
            <p class="card-text">In this paper, we consider the challenge of summarizing patients medical progress notes in a limited data setting. For the Problem List Summarization (shared task 1A) at the BioNLP Workshop 2023, we demonstrate that ClinicalT5 fine-tuned to 765 medical clinic notes outperforms other extractive, abstractive and zero-shot baselines, yielding reasonable baseline systems for medical note summarization. Further, we introduce Hierarchical Ensemble of Summarization Models (HESM), consisting of token-level ensembles of diverse fine-tuned ClinicalT5 models, followed by Minimum Bayes Risk (MBR) decoding. Our HESM approach lead to a considerable summarization performance boost, and when evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which was the best-performing system at the top of the shared task leaderboard.</p>
            <a href="/paper_BioNLP_108.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Evaluating and Improving Automatic Speech Recognition using Severity
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Ryan Whetten, Casey Kennington</h5>
            <p class="card-text">A common metric for evaluating Automatic Speech Recognition (ASR) is Word Error Rate (WER) which solely takes into account discrepancies at the word-level. Although useful, WER is not guaranteed to correlate well with human judgment or performance on downstream tasks that use ASR. Meaningful assessment of ASR mistakes becomes even more important in high-stake scenarios such as health-care. We propose 2 general measures to evaluate the severity of mistakes made by ASR systems, one based on sentiment analysis and another based on text embeddings. We evaluate these measures on simulated patient-doctor conversations using 5 ASR systems. Results show that these measures capture characteristics of ASR errors that WER does not. Furthermore, we train an ASR system incorporating severity and demonstrate the potential for using severity not only in the evaluation, but in the development of ASR. Advantages and limitations of this methodology are analyzed and discussed.</p>
            <a href="/paper_BioNLP_11.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            ELiRF-VRAIN at BioNLP Task 1B: Radiology Report Summarization
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Vicent Ahuir Esteve, Encarna Segarra, Lluis Hurtado</h5>
            <p class="card-text">This paper presents our system at the Radiology Report Summarization Shared Task-1B of the 22nd BioNLP Workshop 2023. Inspired by the work of the BioBART model, we continuously pre-trained a general domain BART model with biomedical data to adapt it to this specific domain. In the pre-training phase, several pre-training tasks are aggregated to inject linguistic knowledge and increase the abstractivity of the generated summaries. We present the results of our models, and also, we have carried out an additional study on the lengths of the generated summaries, which has provided us with interesting information.</p>
            <a href="/paper_BioNLP_110.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            SINAI at RadSum23: Radiology Report Summarization Based on Domain-Specific Sequence-To-Sequence Transformer Model
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Mariia Chizhikova, Manuel Diaz-Galiano, L. Alfonso Urena-Lopez, M. Teresa Martin-Valdivia</h5>
            <p class="card-text">This paper covers participation of the SINAI team in the shared task 1B: Radiology Report Summarization at the BioNLP workshop held on ACL 2023. Our proposal follows a sequence-to-sequence approach which leverages pre-trained multilingual general domain and monolingual biomedical domain pre-trained language models. The best performing system based on domain-specific model reached 33.96 F1RadGraph score which is the fourth best result among the challenge participants. This model was made publicly available on HuggingFace. We also describe an attempt of Proximal Policy Optimization Reinforcement Learning that was made in order to improve the factual correctness measured with F1RadGraph but did not lead to satisfactory results.</p>
            <a href="/paper_BioNLP_111.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            KnowLab at RadSum23: comparing pre-trained language models in radiology report summarization
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Jinge Wu, Daqian Shi, Abul Hasan, Honghan Wu</h5>
            <p class="card-text">This paper presents our contribution to the RadSum23 shared task organized as part of the BioNLP 2023. We compared state-of-the-art generative language models in generating high-quality summaries from radiology reports. A two-stage fine-tuning approach was introduced for utilizing knowledge learnt from different datasets. We evaluated the performance of our method using a variety of metrics, including BLEU, ROUGE, bertscore, CheXbert, and RadGraph. Our results revealed the potentials of different models in summarizing radiology reports and demonstrated the effectiveness of the two-stage fine-tuning approach. We also discussed the limitations and future directions of our work, highlighting the need for better understanding the architecture design&#39;s effect and optimal way of fine-tuning accordingly in automatic clinical summarizations.</p>
            <a href="/paper_BioNLP_112.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            nav-nlp at RadSum23: Abstractive Summarization of Radiology Reports using BART Finetuning
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Sri Macharla, Ashok Madamanchi, Nikhilesh Kancharla</h5>
            <p class="card-text">This paper describes the experiments undertaken and their results as part of the BioNLP 2023 workshop. We took part in Task 1B: Radiology Report Summarization. Multiple runs were submitted for evaluation from solutions utilizing transfer learning from pre-trained transformer models, which were then fine-tuned on MIMIC-III dataset, for abstractive report summarization.</p>
            <a href="/paper_BioNLP_113.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            e-Health CSIRO at RadSum23: Adapting a Chest X-Ray Report Generator to Multimodal Radiology Report Summarisation
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Aaron Nicolson, Jason Dowling, Bevan Koopman</h5>
            <p class="card-text">We describe the participation of team e-Health CSIRO in the BioNLP RadSum task of 2023. This task aims to develop automatic summarisation methods for radiology. The subtask that we participated in was multimodal; the impression section of a report was to be summarised from a given findings section and set of Chest X-rays (CXRs) of a subject&#39;s study. For our method, we adapted an encoder-to-decoder model for CXR report generation to the subtask. e-Health CSIRO placed seventh amongst the participating teams with a RadGraph ER F1 score of 23.9.</p>
            <a href="/paper_BioNLP_114.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Sanjeev Kumar Karn, Rikhiya Ghosh, Kusuma P, Oladimeji Farri</h5>
            <p class="card-text">Instruction-tuned generative large language models (LLMs), such as ChatGPT and Bloomz, possess excellent generalization abilities. However, they face limitations in understanding radiology reports, particularly when generating the IMPRESSIONS section from the FINDINGS section. These models tend to produce either verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to medical text data during training. We present a system that leverages large-scale medical text data for domain-adaptive pre-training of instruction-tuned LLMs, enhancing their medical knowledge and performance on specific medical tasks. We demonstrate that this system performs better in a zero-shot setting compared to several pretrain-and-finetune adaptation methods on the IMPRESSIONS generation task. Furthermore, it ranks 1st among participating systems in Task 1B: Radiology Report Summarization.</p>
            <a href="/paper_BioNLP_116.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            UTSA-NLP at RadSum23: Multi-modal Retrieval-Based Chest X-Ray Report Summarization
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Tongnian Wang, Xingmeng Zhao, Anthony Rios</h5>
            <p class="card-text">Radiology report summarization aims to automatically provide concise summaries of radiology findings, reducing time and errors in manual summaries. However, current methods solely summarize the text, which overlooks critical details in the images. Unfortunately, directly using the images in a multimodal model is difficult. Multimodal models are susceptible to overfitting due to their increased capacity, and modalities tend to overfit and generalize at different rates. Thus, we propose a novel retrieval-based approach that uses image similarities to generate additional text features. We further employ few-shot with chain-of-thought and ensemble techniques to boost performance. Overall, our method achieves state-of-the-art performance in the F1RadGraph score, which measures the factual correctness of summaries. We rank second place in both MIMIC-CXR and MIMIC-III hidden tests among 11 teams.</p>
            <a href="/paper_BioNLP_117.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Gangwoo Kim, Hajung Kim, Lei Ji, Seongsu Bae, chanhwi kim, Mujeen Sung, Hyunjae Kim, Kun Yan, Eric Chang, Jaewoo Kang</h5>
            <p class="card-text">In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema.It enables the model to effectively learn the required knowledge and skills from limited resources in the domain.Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task (Delbrouck et al., 2023), our model benefits from its training across multiple tasks and domains.With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.</p>
            <a href="/paper_BioNLP_118.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Zero-shot Temporal Relation Extraction with ChatGPT
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Chenhan Yuan, Qianqian Xie, Sophia Ananiadou</h5>
            <p class="card-text">The goal of temporal relation extraction is to infer the temporal relation between two events in the document. Supervised models are dominant in this task. In this work, we investigate ChatGPT&#39;s ability on zero-shot temporal relation extraction. We designed three different prompt techniques to break down the task and evaluate ChatGPT. Our experiments show that ChatGPT&#39;s performance has a large gap with that of supervised methods and can heavily rely on the design of prompts. We further demonstrate that ChatGPT can infer more small relation classes correctly than supervised methods. The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency temporal inference.</p>
            <a href="/paper_BioNLP_12.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            VBD-NLP at BioLaySumm Task 1: Explicit and Implicit Key Information Selection for Lay Summarization on Biomedical Long Documents
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Phuc Phan, Tri Tran, Hai-Long Trieu</h5>
            <p class="card-text">We describe our systems participated in the BioLaySumm 2023 Task 1, which aims at automatically generating lay summaries of scientific articles in a simplified way so that its content becomes easier to comprehend for non-expert readers. Our approaches are based on selecting key information by both explicit and implicit strategies. For explicit selection strategies, we conduct extractive summarization based on selecting key sentences for training abstractive summarization models. For implicit selection strategies, we utilize a method based on a factorized energy-based model, which is able to extract important information from long documents to generate summaries and achieve promising results. We build our systems using sequence-to-sequence models, which enable us to leverage powerful and biomedical domain pre-trained language models and apply different strategies to generate lay summaries from long documents. We conducted various experiments to carefully investigate the effects of different aspects of this long-document summarization task such as extracting different document lengths and utilizing different pre-trained language models. We achieve the third rank in the shared task (and the second rank excluding the baseline submission of the organizers).</p>
            <a href="/paper_BioNLP_120.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            APTSumm at BioLaySumm Task 1: Biomedical Breakdown, Improving Readability by Relevancy Based Selection
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: A.S. Poornash, Atharva Deshmukh, Archit Sharma, Sriparna Saha</h5>
            <p class="card-text">In this paper we tackle a lay summarization task which aims to produce lay-summary of biomedical articles. BioLaySumm in the BioNLP Workshop at ACL 2023 (Goldsack et al., 2023), has presented us with this lay summarization task for biomedical articles. Our proposed models provide a three-step abstractive approach for summarizing biomedical articles. Our methodology involves breaking down the original document into distinct sections, generating candidate summaries for each subsection, then finally re-ranking and selecting the top performing paragraph for each section. We run ablation studies to establish that each step in our pipeline is critical for improvement in the quality of lay summary. This model achieved the second-highest rank in terms of readability scores (Luo et al., 2022). Our work distinguishes itself from previous studies by not only considering the content of the paper but also its structure, resulting in more coherent and comprehensible lay summaries. We hope that our model for generating lay summaries of biomedical articles will be a useful resource for individuals across various domains, including academia, industry, and healthcare, who require rapid comprehension of key scientific research.</p>
            <a href="/paper_BioNLP_121.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            NCUEE-NLP at BioLaySumm Task 2: Readability-Controlled Summarization of Biomedical Articles Using the PRIMERA Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Chao-Yi Chen, Jen-Hao Yang, Lung-Hao Lee</h5>
            <p class="card-text">This study describes the model design of the NCUEE-NLP system for BioLaySumm Task 2 at the BioNLP 2023 workshop. We separately fine-tune pretrained PRIMERA models to independently generate technical abstracts and lay summaries of biomedical articles. A total of seven evaluation metrics across three criteria were used to compare system performance. Our best submission was ranked first for relevance, second for readability, and fourth for factuality, tying first for overall performance.</p>
            <a href="/paper_BioNLP_122.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Pathology Dynamics at BioLaySumm: the trade-off between Readability, Relevance, and Factuality in Lay Summarization
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Irfan Al-Hussaini, Austin Wu, Cassie Mitchell</h5>
            <p class="card-text">Lay summarization aims to simplify complex scientific information for non-expert audiences. This paper investigates the trade-off between readability and relevance in the lay summarization of long biomedical documents. We introduce a two-stage framework that attains the best readability metrics in the first subtask of BioLaySumm 2023, with 8.924 FleschKincaid Grade Level and 9.188 DaleChall Readability Score. However, this comes at the cost of reduced relevance and factuality, emphasizing the inherent challenges of balancing readability and content preservation in lay summarization. The first stage generates summaries using a large language model, such as BART with LSG attention. The second stage uses a zero-shot sentence simplification method to improve the readability of the summaries. In the second subtask, a hybrid dataset is employed to train a model capable of generating both lay summaries and abstracts. This approach achieves the best readability score and shares the top overall rank with other leading methods. Our study underscores the importance of developing effective methods for creating accessible lay summaries while maintaining information integrity. Future work will integrate simplification and summary generation within a joint optimization framework that generates high-quality lay summaries that effectively communicate scientific content to a broader audience.</p>
            <a href="/paper_BioNLP_123.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            IKM_Lab at BioLaySumm Task 1: Longformer-based Prompt Tuning for Biomedical Lay Summary Generation
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Yu-Hsuan Wu, Ying-Jia Lin, Hung-Yu Kao</h5>
            <p class="card-text">This paper describes the entry by the Intelligent Knowledge Management (IKM) Laboratory in the BioLaySumm 2023 task1. We aim to transform lengthy biomedical articles into concise, reader-friendly summaries that can be easily comprehended by the general public. We utilized a long-text abstractive summarization longformer model and experimented with several prompt methods for this task. Our entry placed 10th overall, but we were particularly proud to achieve a 3rd place score in the readability evaluation metric.</p>
            <a href="/paper_BioNLP_124.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            MDC at BioLaySumm Task 1: Evaluating GPT Models for Biomedical Lay Summarization
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Oisn Turbitt, Robert Bevan, Mouhamad Aboshokor</h5>
            <p class="card-text">This paper presents our approach to the BioLaySumm Task 1 shared task, held at the BioNLP 2023 Workshop. The effective communication of scientific knowledge to the general public is often limited by the technical language used in research, making it difficult for non-experts to comprehend. To address this issue, lay summaries can be used to explain research findings to non-experts in an accessible form. We conduct an evaluation of autoregressive language models, both general and specialized for the biomedical domain, to generate lay summaries from biomedical research article abstracts. Our findings demonstrate that a GPT-3.5 model combined with a straightforward few-shot prompt produces lay summaries that achieve significantly relevance and factuality compared to those generated by a fine-tuned BioGPT model. However, the summaries generated by the BioGPT model exhibit better readability. Notably, our submission for the shared task achieved 1st place in the competition.</p>
            <a href="/paper_BioNLP_125.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            LHS712EE at BioLaySumm 2023: Using BART and LED to summarize biomedical research articles
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Quancheng Liu, Xiheng Ren, V.G.Vinod Vydiswaran</h5>
            <p class="card-text">As part of our participation in BioLaySumm 2023, we explored the use of large language models (LLMs) to automatically generate concise and readable summaries of biomedical research articles. We utilized pre-trained LLMs to fine-tune our summarization models on two provided datasets, and adapt them to the shared task within the constraints of training time and computational power. Our final models achieved very high relevance and factuality scores on the test set, and ranked among the top five models in the overall performance.</p>
            <a href="/paper_BioNLP_126.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            IITR at BioLaySumm Task 1:Lay Summarization of BioMedical articles using Transformers
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Venkat praneeth Reddy, Pinnapu Reddy Harshavardhan Reddy, Karanam Sai Sumedh, Raksha Sharma</h5>
            <p class="card-text">Initially, we analyzed the datasets in a statistical way so as to learn about various sections&#39; contributions to the final summary in both the pros and life datasets. We found that both the datasets have an Introduction and Abstract along with some initial parts of the results contributing to the summary. We considered only these sections in the next stage of analysis. We found the optimal length or no of sentences of each of the Introduction, abstract, and result which contributes best to the summary. After this statistical analysis, we took the pre-trained model Facebook/bart-base and fine-tuned it with both the datasets PLOS and eLife. While fine-tuning and testing the results we have used chunking because the text lengths are huge. So to not lose information due to the number of token constraints of the model, we used chunking. Finally, we saw the eLife model giving more accurate results than PLOS in terms of readability aspect, probably because the PLOS summary is closer to its abstract, we have considered the eLife model as our final model and tuned the hyperparameters. We are ranked 7th overall and 1st  in readability</p>
            <a href="/paper_BioNLP_127.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            CSIRO Data61 Team at BioLaySumm Task 1: Lay Summarisation of Biomedical Research Articles Using Generative Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Mong Yuan Sim, Xiang Dai, Maciej Rybinski, Sarvnaz Karimi</h5>
            <p class="card-text">Lay summarisation aims at generating a summary for non-expert audience which allows them to keep updated with latest research in a specific field. Despite the significant advancements made in the field of text summarisation, lay summarisation remains relatively under-explored. We present a comprehensive set of experiments and analysis to investigate the effectiveness of existing pre-trained language models in generating lay summaries. When evaluate our models using a BioNLP Shared Task, BioLaySumm, our submission ranked second for the relevance criteria and third overall among 21 competing teams.</p>
            <a href="/paper_BioNLP_128.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Shreya Chandrasekhar, Chieh-Yang Huang, Ting-Hao Huang</h5>
            <p class="card-text">The rapid growth of scientific publications, particularly during the COVID-19 pandemic, emphasizes the need for tools to help researchers efficiently comprehend the latest advancements. One essential part of understanding scientific literature is research aspect classification, which categorizes sentences in abstracts to Background, Purpose, Method, and Finding. In this study, we investigate the impact of different datasets on model performance for the crowd-annotated CODA-19 research aspect classification task. Specifically, we explore the potential benefits of using the large, automatically curated PubMed 200K RCT dataset and evaluate the effectiveness of large language models (LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that using the PubMed 200K RCT dataset does not improve performance for the CODA-19 task. We also observe that while GPT-4 performs well, it does not outperform the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance of a dedicated and task-aligned datasets dataset for the target task.</p>
            <a href="/paper_BioNLP_13.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            ISIKSumm at BioLaySumm Task 1: BART-based Summarization System Enhanced with Bio-Entity Labels
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Cagla Colak, lknur Karadeniz</h5>
            <p class="card-text">Communicating scientific research to the general public is an essential yet challenging task.Lay summaries, which provide a simplified version of research findings, can bridge the gapbetween scientific knowledge and public understanding. The BioLaySumm task (Goldsack et al., 2023) is a shared task that seeks to automate this process by generating lay summaries from biomedical articles. Two different datasets that have been created from curating two biomedical journals (PLOS and eLife) are provided by the task organizers. As a participant in this shared task, we developed a system to generate a lay summary from an article&#39;s abstract and main text.</p>
            <a href="/paper_BioNLP_130.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Sentiment-guided Transformer with Severity-aware Contrastive Learning for Depression Detection on Social Media
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Tianlin Zhang, Kailai Yang, Sophia Ananiadou</h5>
            <p class="card-text">Early identification of depression is beneficial to public health surveillance and disease treatment. There are many models that mainly treat the detection as a binary classification task, such as detecting whether a user is depressed. However, identifying users&#39; depression severity levels from posts on social media is more clinically useful for future prevention and treatment. Existing severity detection methods mainly model the semantic information of posts while ignoring the relevant sentiment information, which can reflect the user&#39;s state of mind and could be helpful for severity detection. In addition, they treat all severity levels equally, making the model difficult to distinguish between closely-labeled categories. We propose a sentiment-guided Transformer model, which efficiently fuses social media posts&#39; semantic information with sentiment information. Furthermore, we also utilize a supervised severity-aware contrastive learning framework to enable the model to better distinguish between different severity levels. The experimental results show that our model achieves superior performance on two public datasets, while further analysis proves the effectiveness of all proposed modules.</p>
            <a href="/paper_BioNLP_14.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Exploring Drug Switching in Patients: A Deep Learning-based Approach to Extract Drug Changes and Reasons from Social Media
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Mourad Sarrouti, Carson Tao, Yoann Mamy Randriamihaja</h5>
            <p class="card-text">Social media (SM) can provide valuable information about patients&#39; experiences with multiple drugs during treatments. Although information extraction from SM has been well-studied, drug switches detection and reasons behind these switches from SM have not been studied yet. Therefore, in this paper, we present a new SM listening approach for analyzing online patient conversations that contain information about drug switching, drug effectiveness, side effects, and adverse drug reactions. We describe a deep learning-based approach for identifying instances of drug switching in SM posts, as well as a method for extracting the reasons behind these switches. To train and test our models, we used annotated SM data from internal dataset which is automatically created using a rule-based method. We evaluated our models using Text-to-Text Transfer Transformer (T5) and found that our SM listening approach can extract medication change information and reasons with high accuracy, achieving  an F1-score of 98% and a ROUGE-1 score of 93%, respectively. Overall, our results suggest that our SM listening approach has the potential to provide valuable insights into patients&#39; experiences with drug treatments, which can be used to improve patient outcomes and the effectiveness of drug treatments.</p>
            <a href="/paper_BioNLP_15.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Is the ranking of PubMed similar articles good enough? An evaluation of text similarity methods for three datasets
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Mariana Neves, Ines Schadock, Beryl Eusemann, Gilbert Schnfelder, Bettina Bert, Daniel Butzke</h5>
            <p class="card-text">The use of seed articles in information retrieval provides many advantages, such as a longercontext and more details about the topic being searched for. Given a seed article (i.e., a PMID), PubMed provides a pre-compiled list of similar articles to support the user in finding equivalent papers in the biomedical literature. We aimed at performing a quantitative evaluation of the PubMed Similar Articles based on three existing biomedical text similarity datasets, namely, RELISH, TREC-COVID, and SMAFIRA-c. Further, we carried out a survey and an evaluation of various text similarity methods on these three datasets. Our experiments considered the original title and abstract from PubMed as well as automatically detected sections and manually annotated relevant sentences. We provide an overview about which methods better performfor each dataset and compare them to the ranking in PubMed similar articles. While resultsvaried considerably among the datasets, we were able to obtain a better performance thanPubMed for all of them. Datasets and source codes are available at: https://github.com/mariananeves/reranking</p>
            <a href="/paper_BioNLP_16.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical Events?
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Laura Zanella, Yannick Toussaint</h5>
            <p class="card-text">Biomedical event extraction can be divided into three main subtasks; (1) biomedical event trigger detection, (2) biomedical argument identification and (3) event construction. This work focuses in the two first subtasks. For the first subtask we analyze a set of transformer language models that are commonly used in the biomedical domain to evaluate and compare their capacity for event trigger detection. We fine-tune the models using seven manually annotated corpora to assess their performance in different biomedical subdomains. SciBERT emerged as the highest performing model, presenting a slight improvement compared to baseline models. Then, for the second subtask we construct a knowledge graph (KG) from the biomedical corpora and integrate its KG embeddings to SciBERT to enrich its semantic information. We demonstrate that adding the KG embeddings to the model improves the argument identification performance by around 20 \%, and by around 15 \% compared to two baseline models. Our results suggest that fine-tuning a transformer model that is pretrained from scratch with biomedical and general data allows to detect event triggers and identify arguments covering different biomedical subdomains, and therefore improving its generalization. Furthermore, the integration of KG embeddings into the model can significantly improve the performance of biomedical event argument identification, outperforming the results of baseline models.</p>
            <a href="/paper_BioNLP_17.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            An end-to-end neural model based on cliques and scopes for frame extraction in long breast radiology reports
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Perceval Wajsburt, Xavier Tannier</h5>
            <p class="card-text">We consider the task of automatically extracting various overlapping frames, i.e, structured entities composed of multiple labels and mentions, from long clinical breast radiology documents. While many methods exist for related topics such as event extraction, slot filling, or discontinuous entity recognition, a challenge in our study resides in the fact that clinical reports typically contain overlapping frames that span multiple sentences or paragraphs.We propose a new method that addresses these difficulties and evaluate it on a new annotated corpus.Despite the small number of documents, we show that the hybridization between knowledge injection and a learning-based system allows us to quickly obtain proper results.We will also introduce the concept of scope relations and show that it both improves the performance of our system, and provides a visual explanation of the predictions.</p>
            <a href="/paper_BioNLP_18.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            DISTANT: Distantly Supervised Entity Span Detection and Classification
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Ken Yano, Makoto Miwa, Sophia Ananiadou</h5>
            <p class="card-text">We propose a distantly supervised pipeline NER which executes entity span detection and entity classification in sequence named DISTANT (DIstantly Supervised enTity spAN deTection and classification).The former entity span detector extracts possible entity mention spans by the distant supervision. Then the later entity classifier assigns each entity span to one of the positive entity types or none by employing a positive and unlabeled (PU) learning framework. Two models were built based on the pre-trained SciBERT model and fine-tuned with the silver corpus generated by the distant supervision.Experimental results on BC5CDR and NCBI-Disease datasets show that our method outperforms the end-to-end NER baselines without PU learning by a large margin. In particular, it increases the recall score effectively.</p>
            <a href="/paper_BioNLP_19.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Gaussian Distributed Prototypical Network for Few-shot Genomic Variant Detection
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Jiarun Cao, Niels Peek, Andrew Renehan, Sophia Ananiadou</h5>
            <p class="card-text">Automatically identifying genetic mutations in the cancer literature using text mining technology has been an important way to study the vast amount of cancer medical literature. However, novel knowledge regarding the genetic variants proliferates rapidly, though current supervised learning models struggle with discovering these unknown entity types. Few-shot learning allows a model to perform effectively with great generalization on new entity types, which has not been explored in recognizing cancer mutation detection. This paper addresses cancer mutation detection tasks with few-shot learning paradigms. We propose GDPN framework, which models the label dependency from the training examples in the support set and approximates the transition scores via Gaussian distribution. The experiments on three benchmark cancer mutation datasets show the effectiveness of our proposed model.</p>
            <a href="/paper_BioNLP_2.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Large Language Models as Instructors: A Study on Multilingual Clinical Entity Extraction
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Simon Meoni, Eric De la Clergerie, Theo Ryffel</h5>
            <p class="card-text">In clinical and other specialized domains, data are scarce due to their confidential nature. This lack of data is a major problem when fine-tuning language models.Nevertheless, very large language models (LLMs) are promising for the medical domain but cannot be used directly in healthcare facilities due to data confidentiality issues. We explore an approach of annotating training data with LLMs to train smaller models more adapted to our problem. We show that this method yields promising results for information extraction tasks.</p>
            <a href="/paper_BioNLP_20.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Event-independent temporal positioning: application to French clinical text
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Nesrine Bannour, Bastien Rance, Xavier Tannier, Aurelie Neveol</h5>
            <p class="card-text">Extracting temporal relations usually entails identifying and classifying the relation between two mentions. However, the definition of temporal mentions strongly depends on the text type and the application domain. Clinical text in particular is complex. It may describe events that occurred at different times, contain redundant information and a variety of domain-specific temporal expressions. In this paper, we propose a novel event-independent representation of temporal relations that is task-independent and, therefore, domain-independent. We are interested in identifying homogeneous text portions from a temporal standpoint and classifying the relation between each text portion and the document creation time. Temporal relation extraction is cast as a sequence labeling task and evaluated on oncology notes. We further evaluate our temporal representation by the temporal positioning of toxicity events of chemotherapy administrated to colon and lung cancer patients described in French clinical reports. An overall macro F-measure of 0.86 is obtained for temporal relation extraction by a neural token classification model trained on clinical texts written in French. Our results suggest that the toxicity event extraction task can be performed successfully by automatically identifying toxicity events and placing them within the patient timeline (F-measure .62). The proposed system has the potential to assist clinicians in the preparation of tumor board meetings.</p>
            <a href="/paper_BioNLP_22.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            ADEQA: A Question Answer based approach for joint ADE-Suspect Extraction using Sequence-To-Sequence Transformers
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Vinayak Arannil, Tomal Deb, Atanu Roy</h5>
            <p class="card-text">Early identification of Adverse Drug Events (ADE) is critical for taking prompt actions while introducing new drugs into the market. These ADEs information are available through various unstructured data sources like clinical study reports, patient health records, social media posts, etc. Extracting ADEs and the related suspect drugs using machine learning is a challenging task due to the complex linguistic relations between drug  ADE pairs in textual data and unavailability of large corpus of labelled datasets. This paper introduces ADEQA, a question- answer(QA) based approach using quasi supervised labelled data and sequence-to-sequence transformers to extract ADEs, drug suspects and the relationships between them. Unlike traditional QA models, natural language generation (NLG) based models don&#39;t require extensive token level labelling and thereby reduces the adoption barrier significantly. On a public ADE corpus, we were able to achieve state-of-the-art results with an F1 score of 94% on establishing the relationships between ADEs and the respective suspects.</p>
            <a href="/paper_BioNLP_24.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Privacy Aware Question-Answering System for Online Mental Health Risk Assessment
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Prateek Chhikara, Ujjwal Pasupulety, John Marshall, Dhiraj Chaurasia, Shweta Kumari</h5>
            <p class="card-text">Social media platforms have enabled individuals suffering from mental illnesses to share their lived experiences and find the online support necessary to cope. However, many users fail to receive genuine clinical support, thus exacerbating their symptoms. Screening users based on what they post online can aid providers in administering targeted healthcare and minimize false positives. Pre-trained Language Models (LMs) can assess users&#39; social media data and classify them in terms of their mental health risk. We propose a Question-Answering (QA) approach to assess mental health risk using the Unified-QA model on two large mental health datasets. To protect user data, we extend Unified-QA by anonymizing the model training process using differential privacy. Our results demonstrate the effectiveness of modeling risk assessment as a QA task, specifically for mental health use cases. Furthermore, the model&#39;s performance decreases by less than 1% with the inclusion of differential privacy. The proposed system&#39;s performance is indicative of a promising research direction that will lead to the development of privacy-aware diagnostic systems.</p>
            <a href="/paper_BioNLP_25.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            AliBERT: A Pre-trained Language Model for French Biomedical Text
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Aman Berhe, Guillaume Draznieks, Vincent Martenot, Valentin Masdeu, Lucas Davy, Jean-Daniel Zucker</h5>
            <p class="card-text">Over the past few years, domain specific pretrained language models have been investigated and have shown remarkable achievements in different downstream tasks, especially in biomedical domain. These achievements stem on the well known BERT architecture which uses an attention based self-supervision for context learning of textual documents. However, these domain specific biomedical pretrained language models mainly use English corpora. Therefore, non-English, domain-specific pretrained models remain quite rare, both of these requirements being hard to achieve. In this work, we proposed AliBERT, a biomedical pretrained language model for French and investigated different learning strategies. AliBERT is trained using regularized Unigram based tokenizer trained for this purpose. AliBERT has achieved state of the art F1 and accuracy scores in different down-stream biomedical tasks. Our pretrained model manages to outperform some French non domain-specific models such as CamemBERT and FlauBERT on diverse down-stream tasks, with less pretraining and training time and with much smaller corpora.</p>
            <a href="/paper_BioNLP_28.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Multiple Evidence Combination for Fact-Checking of Health-Related Information
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Pritam Deka, Anna Jurek-Loughrey, Deepak P</h5>
            <p class="card-text">Fact-checking of health-related claims has become necessary in this digital age, where any information posted online is easily available to everyone. The most effective way to verify such claims is by using evidences obtained from reliable sources of medical knowledge, such as PubMed. Recent advances in the field of NLP have helped automate such fact-checking tasks. In this work, we propose a domain-specific BERT-based model using a transfer learning approach for the task of predicting the veracity of claim-evidence pairs for the verification of health-related facts. We also improvise on a method to combine multiple evidences retrieved for a single claim, taking into consideration conflicting evidences as well. We also show how our model can be exploited when labelled data is available and how back-translation can be used to augment data when there is data scarcity.</p>
            <a href="/paper_BioNLP_29.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Building a Corpus for Biomedical Relation Extraction of Species Mentions
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Oumaima El Khettari, Solen Quiniou, Samuel Chaffron</h5>
            <p class="card-text">We present a manually annotated new corpus, Species-Species Interaction (SSI), for extracting meaningful binary relations between species, in biomedical texts, at sentence level, with a focus on the gut microbiota. The corpus leverages PubTator to annotate species in full-text articles after evaluating different NER species taggers. Our first results are promising for extracting relations between species using BERT and its biomedical variants.</p>
            <a href="/paper_BioNLP_31.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Automated Extraction of Molecular Interactions and Pathway Knowledge using Large Language Model, Galactica: Opportunities and Challenges
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa Lpez-Marrero, Patrick Johnstone, Shinjae Yoo, Francis Alexander</h5>
            <p class="card-text">Understanding protein interactions and pathway knowledge is essential for comprehending living systems and investigating the mechanisms underlying various biological functions and complex diseases. While numerous databases curate such biological data obtained from literature and other sources, they are not comprehensive and require considerable effort to maintain. One mitigation strategies can be utilizing large language models to automatically extract biological information and explore their potential in life science research. This study presents an initial investigation of the efficacy of utilizing a large language model, Galactica in life science research by assessing its performance on tasks involving protein interactions, pathways, and gene regulatory relation recognition. The paper details the results obtained from the model evaluation, highlights the findings, and discusses the opportunities and challenges.</p>
            <a href="/paper_BioNLP_32.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Automatic Glossary of Clinical Terminology: a Large-Scale Dictionary of Biomedical Definitions Generated from Ontological Knowledge
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Franois Remy, Kris Demuynck, Thomas Demeester</h5>
            <p class="card-text">Background: More than 400.000 biomedical concepts and some of their relationships are contained in SnomedCT, a comprehensive biomedical ontology. However, their concept names are not always readily interpretable by non-experts, or patients looking at their own electronic health records (EHR). Clear definitions or descriptions in understandable language or often not available. Therefore, generating human-readable definitions for biomedical concepts might help make the information they encode more accessible and understandable to a wider public.Objective: In this article, we introduce the Automatic Glossary of Clinical Terminology (AGCT), a large-scale biomedical dictionary of clinical concepts generated using high-quality information extracted from the biomedical knowledge contained in SnomedCT.Methods: We generate a novel definition for every SnomedCT concept, after prompting the OpenAI Turbo model, a variant of GPT 3.5, using a high-quality verbalization of the SnomedCT relationships of the to-be-defined concept. A significant subset of the generated definitions was subsequently evaluated by NLP researchers with biomedical expertise on 5-point scales along the following three axes: factuality, insight, and fluency.Results: AGCT contains 422,070 computer-generated definitions for SnomedCT concepts, covering various domains such as diseases, procedures, drugs, and anatomy. The average length of the definitions is 49 words. The definitions were assigned average scores of over 4.5 out of 5 on all three axes, indicating a majority of factual, insightful, and fluent definitions.Conclusion: AGCT is a novel and valuable resource for biomedical tasks that require human-readable definitions for SnomedCT concepts. It can also serve as a base for developing robust biomedical retrieval models or other applications that leverage natural language understanding of biomedical knowledge.</p>
            <a href="/paper_BioNLP_34.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Comparing and combining some popular NER approaches on Biomedical tasks
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Harsh Verma, Sabine Bergler, Narjesossadat Tahaei</h5>
            <p class="card-text">We compare three simple and popular approaches for NER: 1) SEQ (sequence labeling with a linear token classifier) 2) SeqCRF (sequence labeling with Conditional Random Fields), and 3) SpanPred (span prediction with boundary token embeddings). We compare the approaches on 4 biomedical NER tasks: GENIA, NCBI-Disease, LivingNER (Spanish), and SocialDisNER (Spanish). The SpanPred model demonstrates state-of-the-art performance on LivingNER and SocialDisNER, improving F1 by 1.3 and 0.6 F1 respectively. The SeqCRF model also demonstrates state-of-the-art performance on LivingNER and SocialDisNER, improving F1 by 0.2 F1 and 0.7 respectively. The SEQ model is competitive with the state-of-the-art on LivingNER dataset. We explore some simple ways of combining the three approaches. We find that majority voting consistently gives high precision and high F1 across all 4 datasets.Lastly, we implement a system that learns to combine SEQ&#39;s and SpanPred&#39;s predictions, generating systems that give high recall and high F1 across all 4 datasets. On the GENIA dataset, we find that our learned combiner system significantly boosts F1(+1.2) and recall(+2.1) over the systems being combined.</p>
            <a href="/paper_BioNLP_35.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Extracting Drug-Drug and Protein-Protein Interactions from Text using a Continuous Update of Tree-Transformers
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Sudipta Singha Roy, Robert E. Mercer</h5>
            <p class="card-text">Understanding biological mechanisms requires determining mutual protein-protein interactions (PPI). Obtaining drug-drug interactions (DDI) from scientific articles provides important information about drugs. Extracting such medical entity interactions from biomedical articles is challenging due to complex sentence structures. To address this issue, our proposed model utilizes tree-transformers to generate the sentence representation first, and then a sentence-to-word update step to fine-tune the word embeddings which are again used by the tree-transformers to generate enriched sentence representations. Using the tree-transformers helps the model preserve syntactical information and provide semantic information. The fine-tuning provided by the continuous update step adds improved semantics to the representation of each sentence. Our model outperforms other prominent models with a significant performance boost on the five standard PPI corpora and a performance boost on the one benchmark DDI corpus that are used in our experiments.</p>
            <a href="/paper_BioNLP_40.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Resolving Elliptical Compounds in German Medical Text
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Niklas Kammer, Florian Borchert, Silvia Winkler, Gerard de Melo, Matthieu-P. Schapranow</h5>
            <p class="card-text">Elliptical coordinated compound noun phrases (ECCNPs), a special kind of coordination ellipsis, are a common phenomenon in German medical texts. As their presence is known to affect the performance in downstream tasks such as entity extraction and disambiguation, their resolution can be a useful preprocessing step in information extraction pipelines. In this work, we present a new comprehensive dataset of more than 4,000 manually annotated ECCNPs in German medical text, along with the respective ground truth resolutions. Based on this data, we propose a generative encoder-decoder Transformer model, allowing for a simple end-to-end resolution of ECCNPs from raw input strings with very high accuracy (90.5% exact match score). We compare our approach to an elaborate rule-based baseline, which the generative model outperforms by a large margin. We further investigate different scenarios for prompting large language models (LLM) to resolve ECCNPs. In a zero-shot setting, performance is remarkably poor (21.6% exact matches), as the LLM tends to apply complex changes to the inputs unrelated to our specific task. We also find no improvement over the generative model when using the LLM for post-filtering of generated candidate resolutions.</p>
            <a href="/paper_BioNLP_41.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Chandreen Liyanage, Muskan Garg, Vijay Mago, Sunghwan Sohn</h5>
            <p class="card-text">Amid ongoing health crisis, there is a growing necessity to discern possible signs of Wellness Dimensions (WD) manifested in self-narrated text. As the distribution of WD on social media data is intrinsically imbalanced, we experiment the generative AI techniques for data augmentation to enable further improvement in the pre-screening task of classifying WD. To this end, we propose a simple yet effective data augmentation approach through prompt-based Generative AI models, and evaluate the ROUGE scores and syntactic/ semantic similarity among existing interpretations and augmented data. Our approach with ChatGPT model surpasses all the other methods and achieves improvement over baselines such as Easy-Data Augmentation (EDA) and Backtranslation (BT).</p>
            <a href="/paper_BioNLP_42.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            End-to-end clinical temporal information extraction with multi-head attention
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Timothy Miller, Steven Bethard, Dmitriy Dligach, Guergana Savova</h5>
            <p class="card-text">Understanding temporal relationships in text from electronic health records can be valuable for many important downstream clinical applications. Since Clinical TempEval 2017, there has been little work on end-to-end systems for temporal relation extraction, with most work focused on the setting where gold standard events and time expressions are given. In this work, we make use of a novel multi-headed attention mechanism on top of a pre-trained transformer encoder to allow the learning process to attend to multiple aspects of the contextualized embeddings. Our system achieves state of the art results on the THYME corpus by a wide margin, in both the in-domain and cross-domain settings.</p>
            <a href="/paper_BioNLP_43.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Intermediate Domain Finetuning for Weakly Supervised Domain-adaptive Clinical NER
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Shilpa Suresh, Nazgol Tavabi, Shahriar Golchin, Leah Gilreath, Rafael Garcia-Andujar, Alexander Kim, Joseph Murray, Blake Bacevich, Ata Kiapour</h5>
            <p class="card-text">Accurate human-annotated data for real-worlduse cases can be scarce and expensive to obtain.In the clinical domain, obtaining such data is evenmore difficult due to privacy concerns which notonly restrict open access to quality data but also require that the annotation be done by domain experts.In this paper, we propose a novel framework - InterDAPT - that leverages Intermediate Domain Finetuning to allow language models to adapt to narrow domains with small, noisy datasets. By making use of peripherally-related, unlabeled datasets,this framework circumvents domain-specific datascarcity issues. Our results show that this weaklysupervised framework provides performance improvements in downstream clinical named entityrecognition tasks.</p>
            <a href="/paper_BioNLP_44.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang</h5>
            <p class="card-text">ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT&#39;s pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data.</p>
            <a href="/paper_BioNLP_46.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Vera Pavlova, Mohammed Makhlouf</h5>
            <p class="card-text">Using language models (LMs) pre-trained in a self-supervised setting on large corpora and then fine-tuning for a downstream task has helped to deal with the problem of limited label data for supervised learning tasks such as Named Entity Recognition (NER). Recent research in biomedical language processing has offered a number of biomedical LMs pre-trained using different methods and techniques that advance results on many BioNLP tasks, including NER. However, there is still a lack of a comprehensive comparison of pre-training approaches that would work more optimally in the biomedical domain. This paper aims to investigate different pre-training methods, such as pre-training the biomedical LM from scratch and pre-training it in a continued fashion. We compare existing methods with our proposed pre-training method of initializing weights for new tokens by distilling existing weights from the BERT model inside the context where the tokens were found. The method helps to speed up the pre-training stage and improve performance on NER. In addition, we compare how masking rate, corruption strategy, and masking strategies impact the performance of the biomedical LM. Finally, using the insights from our experiments, we introduce a new biomedical LM (BIOptimus), which is pre-trained using Curriculum Learning (CL) and contextualized weight distillation method. Our model sets new states of the art on several biomedical Named Entity Recognition (NER) tasks. We release our code and all pre-trained models.</p>
            <a href="/paper_BioNLP_47.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Biomedical Language Models are Robust to Sub-optimal Tokenization
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Bernal Jimenez Gutierrez, Huan Sun, Yu Su</h5>
            <p class="card-text">As opposed to general English, many concepts in biomedical terminology have been designed in recent history by biomedical professionals with the goal of being precise and concise. This is often achieved by concatenating meaningful biomedical morphemes to create new semantic units. Nevertheless, most modern biomedical language models (LMs) are pre-trained using standard domain-specific tokenizers derived from large scale biomedical corpus statistics without explicitly leveraging the agglutinating nature of biomedical language. In this work, we first find that standard open-domain and biomedical tokenizers are largely unable to segment biomedical terms into meaningful components. Therefore, we hypothesize that using a tokenizer which segments biomedical terminology more accurately would enable biomedical LMs to improve their performance on downstream biomedical NLP tasks, especially ones which involve biomedical terms directly such as named entity recognition (NER) and entity linking. Surprisingly, we find that pre-training a biomedical LM using a more accurate biomedical tokenizer does not improve the entity representation quality of a language model as measured by several intrinsic and extrinsic measures such as masked language modeling prediction (MLM) accuracy as well as NER and entity linking performance. These quantitative findings, along with a case study which explores entity representation quality more directly, suggest that the biomedical pre-training process is quite robust to instances of sub-optimal tokenization.</p>
            <a href="/paper_BioNLP_48.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Distantly Supervised Document-Level Biomedical Relation Extraction with Neighborhood Knowledge Graphs
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Takuma Matsubara, Makoto Miwa, Yutaka Sasaki</h5>
            <p class="card-text">We propose a novel distantly supervised document-level biomedical relation extraction model that uses partial knowledge graphs that include the graph neighborhood of the entities appearing in each input document. Most conventional distantly supervised relation extraction methods use only the entity relations automatically annotated by using knowledge base entries. They do not fully utilize the rich information in the knowledge base, such as entities other than the target entities and the network of heterogeneous entities defined in the knowledge base. To address this issue, our model integrates the representations of the entities acquired from the neighborhood knowledge graphs with the representations of the input document. We conducted experiments on the ChemDisGene dataset using Comparative Toxicogenomics Database (CTD) for document-level relation extraction with respect to interactions between drugs, diseases, and genes. Experimental results confirmed the performance improvement by integrating entities and their neighborhood biochemical information from the knowledge base.</p>
            <a href="/paper_BioNLP_49.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Exploring Partial Knowledge Base Inference in Biomedical Entity Linking
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Hongyi Yuan, Keming Lu, Zheng Yuan</h5>
            <p class="card-text">Biomedical entity linking (EL) consists of named entity recognition (NER) and named entity disambiguation (NED). EL models are trained on corpora labeled by a predefined KB. However, it is a common scenario that only entities within a subset of the KB are precious to stakeholders. We name this scenario partial knowledge base inference; training an EL model with one KB and inferring on the part of it without further training. In this work, we give a detailed definition and evaluation procedures for this practically valuable but significantly understudied scenario and evaluate methods from three representative EL paradigms. We construct partial KB inference benchmarks and witness a catastrophic degradation in EL performance due to dramatically precision drop.Our findings reveal these EL paradigms can not correctly handle unlinkable mentions (NIL), so they are not robust to partial KB inference. We also propose two simple-and-effective redemption methods to combat the NIL issue with little computational overhead.</p>
            <a href="/paper_BioNLP_5.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            BioNART: A Biomedical Non-AutoRegressive Transformer for Natural Language Generation
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Masaki Asada, Makoto Miwa</h5>
            <p class="card-text">We propose a novel Biomedical domain-specific Non-AutoRegressive Transformer model for natural language generation: BioNART. Our BioNART is based on an encoder-decoder model, and both encoder and decoder are compatible with widely used BERT architecture, which allows benefiting from publicly available pre-trained biomedical language model checkpoints. We performed additional pre-training and fine-tuned BioNART on biomedical summarization and doctor-patient dialogue tasks. Experimental results show that our BioNART achieves about 94% of the ROUGE score to the pre-trained autoregressive model while realizing an 18 times faster inference speed on the iCliniq dataset.</p>
            <a href="/paper_BioNLP_50.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Biomedical Relation Extraction with Entity Type Markers and Relation-specific Question Answering
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Koshi Yamada, Makoto Miwa, Yutaka Sasaki</h5>
            <p class="card-text">Recently, several methods have tackled the relation extraction task with QA and have shown successful results. However, the effectiveness of existing methods in specific domains, such as the biomedical domain, is yet to be verified. When there are multiple entity pairs that share an entity in a sentence, a QA-based relation extraction model that outputs only one single answer to a given question may not extract desired relations. In addition, these methods employ QA models that are not tuned for relation extraction. To address these issues, we first extend and apply a span QA-based relation extraction method to the drug-protein relation extraction by creating question templates and incorporating entity type markers. We further propose a binary QA-based method that directly uses the entity information available in the relation extraction task. The experimental results on the DrugProt dataset show that our QA-based methods, especially the proposed binary QA method, are effective for drug-protein relation extraction.</p>
            <a href="/paper_BioNLP_51.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Biomedical Document Classification with Literature Graph Representations of Bibliographies and Entities
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Ryuki Ida, Makoto Miwa, Yutaka Sasaki</h5>
            <p class="card-text">This paper proposes a new document classification method that incorporates the representations of a literature graph created from bibliographic and entity information.Recently, document classification performance has been significantly improved with large pre-trained language models; however, there still remain documents that are difficult to classify. External information, such as bibliographic information, citation links, descriptions of entities, and medical taxonomies, has been considered one of the keys to dealing with such documents in document classification. Although several document classification methods using external information have been proposed, they only consider limited relationships, e.g., word co-occurrence and citation relationships. However, there are multiple types of external information.To overcome the limitation of the conventional use of external information, we propose a document classification model that simultaneously considers bibliographic and entity information to deeply model the relationships among documents using the representations of the literature graph.The experimental results show that our proposed method outperforms existing methods on two document classification datasets in the biomedical domain with the help of the literature graph.</p>
            <a href="/paper_BioNLP_53.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: David Kartchner, Selvi Ramalingam, Irfan Al-Hussaini, Olivia Kronick, Cassie Mitchell</h5>
            <p class="card-text">Meta-analysis of randomized clinical trials (RCTs) plays a crucial role in evidence-based medicine but can be labor-intensive and error-prone. This study explores the use of large language models to enhance the efficiency of aggregating results from randomized clinical trials (RCTs) at scale. We perform a detailed comparison of the performance of these models in zero-shot prompt-based information extraction from a diverse set of RCTs to traditional manual annotation methods. We analyze the results for two different meta-analyses aimed at drug repurposing in cancer therapy pharmacovigilience in chronic myeloid leukemia. Our findings reveal that the best model for the two demonstrated tasks, ChatGPT can generally extract correct information and identify when the desired information is missing from an article. We additionally conduct a systematic error analysis, documenting the prevalence of diverse error types encountered during the process of prompt-based information extraction.</p>
            <a href="/paper_BioNLP_55.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Can Social Media Inform Dietary Approaches for Health Management? A Dataset and Benchmark for Low-Carb Diet
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Skyler Zou, Xiang Dai, Grant Brinkworth, Pennie Taylor, Sarvnaz Karimi</h5>
            <p class="card-text">Social media offers an accessible avenue for individuals of diverse backgrounds and circumstances to share their unique perspectives and experiences. Our study focuses on the experience of low carbohydrate diets, motivated by recent research and clinical trials that elucidates the diet&#39;s promising health benefits. Given the lack of any suitable annotated dataset in this domain, we first define an annotation schema that reflects the interests of healthcare professionals and then manually annotate data from the Reddit social network.Finally, we benchmark the effectiveness of several classification approaches that are based on statistical Support Vector Machines (SVM) classifier, pre-train-then-finetune RoBERTa classifier, and, off-the-shelf ChatGPT API, on our annotated dataset.Our annotations and scripts that are used to download the Reddit posts are publicly available at https://data.csiro.au/collection/csiro:59208.</p>
            <a href="/paper_BioNLP_56.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Promoting Fairness in Classification of Quality of Medical Evidence
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Simon Suster, Timothy Baldwin, Karin Verspoor</h5>
            <p class="card-text">Automatically rating the quality of published research is a critical step in medical evidence synthesis. While several methods have been proposed, their algorithmic fairness has been   overlooked even though significant risks may follow when such systems are deployed in biomedical contexts. In this work, we study fairness on two systems along two sensitive attributes, participant sex and medical area. In some cases, we find important inequalities, leading us to apply various debiasing methods. Upon examining an interplay of systems&#39; predictive performance, fairness, as well as medically critical selective classification capabilities and calibration performance, we find that fairness can sometimes improve through debiasing, but at a cost in other performance measures.</p>
            <a href="/paper_BioNLP_57.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            WeLT: Improving Biomedical Fine-tuned Pre-trained Language Models with Cost-sensitive Learning
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Ghadeer Mobasher, Wolfgang Mller, Olga Krebs, Michael Gertz</h5>
            <p class="card-text">Fine-tuning biomedical pre-trained language models (BioPLMs) such as BioBERT has become a common practice dominating leaderboards across various natural language processing tasks. Despite their success and wide adoption, prevailing fine-tuning approaches for named entity recognition (NER)  naively train BioPLMs on targeted datasets without considering class distributions. This is problematic especially when dealing with imbalanced biomedical gold-standard datasets for NER in which most biomedical entities are underrepresented.In this paper, we address the class imbalance problem and propose WeLT, a cost-sensitive fine-tuning approach based on new re-scaled class weights for the task of biomedical NER. We evaluate WeLT&#39;s fine-tuning performance on mixed-domain and domain-specific BioPLMs using eight biomedical gold-standard datasets. We compare our approach against vanilla fine-tuning and three other existing re-weighting schemes. Our results show the positive impact of handling the class imbalance problem. WeLT outperforms all the vanilla fine-tuned models. Furthermore, our method demonstrates advantages over other existing weighting schemes in most experiments.</p>
            <a href="/paper_BioNLP_58.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Hospital Discharge Summarization Data Provenance
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Paul Landes, Aaron Chaise, Kunal Patel, Sean Huang, Barbara Di Eugenio</h5>
            <p class="card-text">Summarization of medical notes has been studied for decades with hospital discharge summaries garnering recent interest in the research community. While methods for summarizing these notes have been the focus, there has been little work in understanding the feasibility of this task. We believe this effort is warranted given the notes&#39; length and complexity, and that they are often riddled with poorly formatted structured data and redundancy in copy and pasted text. In this work, we investigate the feasibility of the summarization task by finding the origin, or data provenance, of the discharge summary&#39;s source text. As a motivation to understanding the data challenges of the summarization task, we present DSProv, a new dataset of 51 hospital admissions annotated by clinical informatics physicians. The dataset is analyzed for semantics and the extent of copied text from human authored electronic health record (EHR) notes. We also present a novel unsupervised method of matching notes used in discharge summaries, and release our annotation dataset1 and source code to the community.</p>
            <a href="/paper_BioNLP_59.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Dave Van Veen, Cara Van Uden, Maayane Attias, Anuj Pareek, Christian Bluethgen, Malgorzata Polacin, Wah Chiu, Jean-Benoit Delbrouck, Juan Zambrano Chaves, Curtis Langlotz, Akshay Chaudhari, John Pauly</h5>
            <p class="card-text">We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, or clinical text) and via discrete prompting or parameter-efficient fine-tuning. Our results consistently achieve best performance by maximally adapting to the task via pretraining on clinical text and fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developing effective natural language processing solutions for clinical tasks.</p>
            <a href="/paper_BioNLP_61.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients&#39; Active Diagnoses and Problems from Electronic Health Record Progress Notes
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Yanjun Gao, Dmitriy Dligach, Timothy Miller, Majid Afshar</h5>
            <p class="card-text">The BioNLP Workshop 2023 initiated the launch of a shared task on Problem List Summarization (ProbSum) in January 2023. The aim of this shared task is to attract future research efforts in building NLP models for real-world diagnostic decision support applications, where a system generating relevant and accurate diagnoses will augment the healthcare providers&#39; decision-making process and improve the quality of care for patients. The goal for participants is to develop models that generated a list of diagnoses and problems using input from the daily care notes collected from the hospitalization of critically ill patients. Eight teams submitted their final systems to the shared task leaderboard. In this paper, we describe the tasks, datasets, evaluation metrics, and baseline systems. Additionally, the techniques and results of the evaluation of the different approaches tried by the participating teams are summarized.</p>
            <a href="/paper_BioNLP_62.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            BioLaySumm 2023 Shared Task: Lay Summarisation of Biomedical Research Articles
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Tomas Goldsack, Zheheng Luo, Qianqian Xie, Carolina Scarton, Matthew Shardlow, Sophia Ananiadou, Chenghua Lin</h5>
            <p class="card-text">This paper presents the results of the shared task on Lay Summarisation of Biomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at ACL 2023.  The goal of this shared task is to develop abstractive summarisation models capable of generating ``lay summaries&#39;&#39; (i.e., summaries that are comprehensible to non-technical audiences) in both a controllable and non-controllable setting.There are two subtasks: 1) Lay Summarisation, where the goal is for participants to build models for lay summary generation only, given the full article text and the corresponding abstract as input; and2) Readability-controlled Summarisation, where the goal is for participants to train models to generate both the technical abstract and the lay summary, given an article&#39;s main text as input.In addition to overall results, we report on the setup and insights from the BioLaySumm shared task, which attracted a total of 20 participating teams across both subtasks.</p>
            <a href="/paper_BioNLP_63.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Overview of the RadSum23 Shared Task on Multi-modal and Multi-anatomical Radiology Report Summarization
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Jean-Benoit Delbrouck, Maya Varma, Pierre Chambon, Curtis Langlotz</h5>
            <p class="card-text">Radiology report summarization is a growing area of research. Given the Findings and/or Background sections of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. Recent efforts have released systems that achieve promising performance as measured by widely used summarization metrics such as BLEU and ROUGE. However, the research area of radiology report summarization currently faces two important limitations. First, most of the results are reported on private datasets. This limitation prevents the ability to reproduce results and fairly compare different systems and solutions. Secondly, to the best of our knowledge, most research is carried out on chest X-rays. To palliate these two limitations, we propose a radiology report summarization (RadSum) challenge on i) a new dataset of eleven different modalities and anatomies pairs based on the MIMIC-III database ii) a multimodal report summarization dataset based on MIMIC-CXR enhanced with a brand-new test-set from Stanford Hospital. In total, we received 112 submissions across 11 teams.</p>
            <a href="/paper_BioNLP_64.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Boosting Radiology Report Generation by Infusing Comparison Prior
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Sanghwan Kim, Farhad Nooralahzadeh, Morteza Rohanian, Koji Fujimoto, Mizuho Nishio, Ryo Sakamoto, Fabio Rinaldi, Michael Krauthammer</h5>
            <p class="card-text">Recent transformer-based models have made significant strides in generating radiology reports from chest X-ray images. However, a prominent challenge remains; these models often lack prior knowledge, resulting in the generation of synthetic reports that mistakenly reference non-existent prior exams. This discrepancy can be attributed to a knowledge gap between radiologists and the generation models. While radiologists possess patient-specific prior information, the models solely receive X-ray images at a specific time point. To tackle this issue, we propose a novel approach that leverages a rule-based labeler to extract comparison prior information from radiology reports. This extracted comparison prior is then seamlessly integrated into state-of-the-art transformer-based models, enabling them to produce more realistic and comprehensive reports. Our method is evaluated on English report datasets, such as IU X-ray and MIMIC-CXR. The results demonstrate that our approach surpasses baseline models in terms of natural language generation metrics. Notably, our model generates reports that are free from false references to non-existent prior exams, setting it apart from previous models. By addressing this limitation, our approach represents a significant step towards bridging the gap between radiologists and generation models in the domain of medical report generation.</p>
            <a href="/paper_BioNLP_7.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
      <div class="col-12" style="margin-bottom: 15px;">
        <div class="card">
          <div class="card-header">
            Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints
          </div>
          <div class="card-body">
            <h5 class="card-title">Authors: Omid Rohanian, Hannah Jauncey, Mohammadmahdi Nouriborji, Vinod Kumar, Bronner P. Gonalves, Christiana Kartsonaki, ISARIC Clinical Characterisation Group, Laura Merson, David Clifton</h5>
            <p class="card-text">Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP.  In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not. Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining.</p>
            <a href="/paper_BioNLP_9.html" class="btn btn-primary">Go to Paper</a>
          </div>
        </div>
      </div>
      
    </div>


    <script src="static/js/time-extend.js"></script>
    <script>
      $(document).ready(() => {
        add_local_tz('.session_times');
      })
    </script>

    
      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-9X8H03BYC2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-9X8H03BYC2");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-left">
          <img src="static/images/acl2023/acl-logo-2023.png" height="45px"
            width="auto" align="center">
          <span class="lead">ACL 2023</span>
        </p>
        <p class="float-right"><a href="#" class="text-dark">Back to Top</a></p>
        <p class="text-center"> 2023 Association for Computational Linguistics</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
    <div class="modal left fade" id="chatsModal" tabindex="" role="dialog" aria-labelledby="exampleModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-dialog-scrollable" role="document" style="max-height: 100% !important;">
        <div class="modal-content" style="max-height: 100% !important;">
            <div class="modal-header">
                <h4 class="modal-title" id="exampleModalLabel">Active Chats</h4>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
            </div>
            <div class="modal-body">
                <div style="margin-bottom: 1em;">
    <div class="stats-last-update text-muted" id="highly-active-chats-last-update"></div>
    <a id="highly-active-chats-btn-refresh" href="#" class="text-primary" style="display: none;">Refresh now</a>
</div>

<div id="highly-active-chats-list">
</div>

<div class="text-center" id="highly-active-chats-progress-bar" style="margin-bottom: 80em;">
    <div class="spinner-border text-primary" style="margin-top: 3em; width: 3rem; height: 3rem;" role="status">
      <span class="sr-only">Loading...</span>
    </div>
</div>

<br/>
<p class="text-muted">
    <strong>How it works: </strong>
    We calculate the number of new messages for every channel in the last N seconds. Then, we sort them descendingly.
    Channels with no new messages will be randomly shuffled. Please note that the number of messages might not be accurate.
</p>

<script src="static/js/highly-active-chats.js"></script>
<script>
    let channel_stats_server = "https://emnlp2020-channels-stats.azure-api.net"
    $(document).ready( function () {
        $('[data-toggle="tooltip"]').tooltip();
        //load_stats();
    });
</script>

            </div>

            <div class="modal-footer">
                <button type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button>
            </div>
        </div>
    </div>
</div>
</body>

</html>